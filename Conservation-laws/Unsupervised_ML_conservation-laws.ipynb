{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1AIBFGmfuXD"
   },
   "source": [
    "# Notebook for unsupervised ML for conservation laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w29Ce-zx-AVD"
   },
   "outputs": [],
   "source": [
    "# Code taken and adapted from: https://github.com/KindXiaoming/aipoincare/blob/master/backend/training.py\n",
    "# Dataset from: https://github.com/KindXiaoming/aipoincare/blob/master/backend/data/oned_harmonic\n",
    "# ChatGPT assistance: https://chatgpt.com/share/6882476e-0d50-8011-b66e-4d9e376e7696\n",
    "\n",
    "# In case some packages in correct version should be installed, there is a requirements.txt in working_tree, and a code cell below for \"!pip install ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17999,
     "status": "ok",
     "timestamp": 1753368346116,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "mhumPO4CY7BY",
    "outputId": "facc6095-f1b9-4a87-b51e-23b652b2a0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1a. Mount Google Drive (do this once per Colab session)\n",
    "# ------------------------------------------------------------\n",
    "import pickle\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "dir_path = '/content/drive/MyDrive/AI-in-physics_AJP_2025/Conservation-laws/' # needs to be adapted to specific context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Dg9zo77CuXb"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1b. If you are on a local machine, set dir to current working tree\n",
    "# ------------------------------------------------------------\n",
    "import pickle\n",
    "dir_path = '../Conservation-laws/' # needs to be adapted to specific context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1c. If you use the binder version\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "dir_path = '' # needs to be adapted to specific context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8231,
     "status": "ok",
     "timestamp": 1753368356746,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "bFyST4FHYq9m"
   },
   "outputs": [],
   "source": [
    "# --------------------- Import necessary libraries ---------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1753368326256,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "Gx0fZnu24Tj1"
   },
   "outputs": [],
   "source": [
    "# --------------------- Set hyperparameters and data config ---------------------\n",
    "\n",
    "# Whether to remove PCA components below a noise threshold or keep them (normalize instead)\n",
    "prepcaremove = 'no'  # 'no' = normalize all components, 'yes' = discard low-variance ones\n",
    "# Variance threshold for PCA dimension filtering\n",
    "noise_threshold = 0.001\n",
    "# Hidden layer widths for the neural network; first and last layer are added later\n",
    "nn_widths = [256, 256]  # Two hidden layers: 64 units and 32 units\n",
    "hidden_depth = len(nn_widths)\n",
    "# Negative slope for LeakyReLU activation function\n",
    "slope = 0.1\n",
    "# List of noise magnitudes (sigmal) to test during training\n",
    "sigmals = [1e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1.0, 10.0]\n",
    "# Optimizer type: 'Adam' or 'SGD'\n",
    "opt = 'Adam'\n",
    "# Learning rate for optimizer\n",
    "lr = 0.001\n",
    "# Mini-batch size for training\n",
    "batch_size = 64\n",
    "# Number of training iterations (epochs)\n",
    "epoch = 5000\n",
    "# ERD parameter `a` for computing effective dimensionality\n",
    "a = 2\n",
    "# Path to the input data file (must be a .txt file of shape [n_samples, n_features])\n",
    "model = 'harmonic_1d.txt'\n",
    "# Number of random walks to simulate for post-training variance analysis\n",
    "n_walk = 600\n",
    "# Frequency of logging during training\n",
    "log = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1753368357268,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "lbBFI2Ws5IPH"
   },
   "outputs": [],
   "source": [
    "# --------------------- Load and preprocess the data ---------------------\n",
    "\n",
    "# Load dataset as NumPy array\n",
    "xs = np.loadtxt(os.path.join(dir_path, model).replace(\"\\\\\", \"/\"))\n",
    "n_train = xs.shape[0]       # Number of samples\n",
    "input_dim = xs.shape[1]     # Dimensionality of input features\n",
    "\n",
    "# Step 1: Normalize each input feature to zero mean and unit variance\n",
    "xs = (xs - np.mean(xs, axis=0)[np.newaxis, :]) / np.std(xs, axis=0)[np.newaxis, :]\n",
    "\n",
    "# Step 2: Apply PCA (rotation only, no scaling) to decorrelate dimensions\n",
    "pca = PCA()\n",
    "xs = pca.fit_transform(xs)\n",
    "\n",
    "# Step 3: Remove or normalize dimensions based on explained variance\n",
    "remove_dim = 0\n",
    "if prepcaremove == \"no\":\n",
    "    # Keep all dimensions, normalize each one by variance + threshold\n",
    "    xs = xs / (np.std(xs, axis=0)[np.newaxis, :] + noise_threshold)\n",
    "else:\n",
    "    # Keep only dimensions where variance > threshold\n",
    "    input_dim_orig = input_dim\n",
    "    input_dim = np.sum(pca.explained_variance_ratio_ > noise_threshold)\n",
    "    remove_dim = input_dim_orig - input_dim\n",
    "    xs = xs[:, :input_dim]\n",
    "    xs = xs / (np.std(xs, axis=0)[np.newaxis, :])\n",
    "\n",
    "# Add input/output layers to the neural network layer list\n",
    "nn_widths.insert(0, input_dim)\n",
    "nn_widths.append(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84638,
     "status": "ok",
     "timestamp": 1753368490960,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "PTseZnWX5VYV",
    "outputId": "02d42dbb-fa3a-4174-e507-c33f06b8d384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmal = 0.001\n",
      "Epoch 0 | Loss: 0.0180\n",
      "Epoch 200 | Loss: 0.0000\n",
      "Epoch 400 | Loss: 0.0000\n",
      "Epoch 600 | Loss: 0.0000\n",
      "Epoch 800 | Loss: 0.0000\n",
      "Epoch 1000 | Loss: 0.0000\n",
      "Epoch 1200 | Loss: 0.0000\n",
      "Epoch 1400 | Loss: 0.0000\n",
      "Epoch 1600 | Loss: 0.0000\n",
      "Epoch 1800 | Loss: 0.0000\n",
      "Epoch 2000 | Loss: 0.0000\n",
      "Epoch 2200 | Loss: 0.0000\n",
      "Epoch 2400 | Loss: 0.0000\n",
      "Epoch 2600 | Loss: 0.0000\n",
      "Epoch 2800 | Loss: 0.0000\n",
      "Epoch 3000 | Loss: 0.0000\n",
      "Epoch 3200 | Loss: 0.0000\n",
      "Epoch 3400 | Loss: 0.0000\n",
      "Epoch 3600 | Loss: 0.0000\n",
      "Epoch 3800 | Loss: 0.0000\n",
      "Epoch 4000 | Loss: 0.0000\n",
      "Epoch 4200 | Loss: 0.0000\n",
      "Epoch 4400 | Loss: 0.0000\n",
      "Epoch 4600 | Loss: 0.0000\n",
      "Epoch 4800 | Loss: 0.0000\n",
      "sigmal = 0.01\n",
      "Epoch 0 | Loss: 0.0053\n",
      "Epoch 200 | Loss: 0.0001\n",
      "Epoch 400 | Loss: 0.0002\n",
      "Epoch 600 | Loss: 0.0002\n",
      "Epoch 800 | Loss: 0.0001\n",
      "Epoch 1000 | Loss: 0.0002\n",
      "Epoch 1200 | Loss: 0.0001\n",
      "Epoch 1400 | Loss: 0.0001\n",
      "Epoch 1600 | Loss: 0.0001\n",
      "Epoch 1800 | Loss: 0.0001\n",
      "Epoch 2000 | Loss: 0.0001\n",
      "Epoch 2200 | Loss: 0.0001\n",
      "Epoch 2400 | Loss: 0.0001\n",
      "Epoch 2600 | Loss: 0.0001\n",
      "Epoch 2800 | Loss: 0.0001\n",
      "Epoch 3000 | Loss: 0.0001\n",
      "Epoch 3200 | Loss: 0.0001\n",
      "Epoch 3400 | Loss: 0.0001\n",
      "Epoch 3600 | Loss: 0.0001\n",
      "Epoch 3800 | Loss: 0.0001\n",
      "Epoch 4000 | Loss: 0.0001\n",
      "Epoch 4200 | Loss: 0.0001\n",
      "Epoch 4400 | Loss: 0.0001\n",
      "Epoch 4600 | Loss: 0.0001\n",
      "Epoch 4800 | Loss: 0.0001\n",
      "sigmal = 0.03\n",
      "Epoch 0 | Loss: 0.0265\n",
      "Epoch 200 | Loss: 0.0011\n",
      "Epoch 400 | Loss: 0.0010\n",
      "Epoch 600 | Loss: 0.0010\n",
      "Epoch 800 | Loss: 0.0009\n",
      "Epoch 1000 | Loss: 0.0008\n",
      "Epoch 1200 | Loss: 0.0006\n",
      "Epoch 1400 | Loss: 0.0005\n",
      "Epoch 1600 | Loss: 0.0005\n",
      "Epoch 1800 | Loss: 0.0004\n",
      "Epoch 2000 | Loss: 0.0005\n",
      "Epoch 2200 | Loss: 0.0007\n",
      "Epoch 2400 | Loss: 0.0005\n",
      "Epoch 2600 | Loss: 0.0006\n",
      "Epoch 2800 | Loss: 0.0005\n",
      "Epoch 3000 | Loss: 0.0005\n",
      "Epoch 3200 | Loss: 0.0005\n",
      "Epoch 3400 | Loss: 0.0004\n",
      "Epoch 3600 | Loss: 0.0005\n",
      "Epoch 3800 | Loss: 0.0005\n",
      "Epoch 4000 | Loss: 0.0006\n",
      "Epoch 4200 | Loss: 0.0005\n",
      "Epoch 4400 | Loss: 0.0006\n",
      "Epoch 4600 | Loss: 0.0004\n",
      "Epoch 4800 | Loss: 0.0007\n",
      "sigmal = 0.1\n",
      "Epoch 0 | Loss: 0.0294\n",
      "Epoch 200 | Loss: 0.0059\n",
      "Epoch 400 | Loss: 0.0056\n",
      "Epoch 600 | Loss: 0.0055\n",
      "Epoch 800 | Loss: 0.0067\n",
      "Epoch 1000 | Loss: 0.0077\n",
      "Epoch 1200 | Loss: 0.0054\n",
      "Epoch 1400 | Loss: 0.0059\n",
      "Epoch 1600 | Loss: 0.0044\n",
      "Epoch 1800 | Loss: 0.0064\n",
      "Epoch 2000 | Loss: 0.0043\n",
      "Epoch 2200 | Loss: 0.0057\n",
      "Epoch 2400 | Loss: 0.0050\n",
      "Epoch 2600 | Loss: 0.0047\n",
      "Epoch 2800 | Loss: 0.0062\n",
      "Epoch 3000 | Loss: 0.0059\n",
      "Epoch 3200 | Loss: 0.0068\n",
      "Epoch 3400 | Loss: 0.0060\n",
      "Epoch 3600 | Loss: 0.0041\n",
      "Epoch 3800 | Loss: 0.0051\n",
      "Epoch 4000 | Loss: 0.0038\n",
      "Epoch 4200 | Loss: 0.0042\n",
      "Epoch 4400 | Loss: 0.0053\n",
      "Epoch 4600 | Loss: 0.0057\n",
      "Epoch 4800 | Loss: 0.0055\n",
      "sigmal = 0.3\n",
      "Epoch 0 | Loss: 0.0985\n",
      "Epoch 200 | Loss: 0.0469\n",
      "Epoch 400 | Loss: 0.0425\n",
      "Epoch 600 | Loss: 0.0391\n",
      "Epoch 800 | Loss: 0.0564\n",
      "Epoch 1000 | Loss: 0.0441\n",
      "Epoch 1200 | Loss: 0.0526\n",
      "Epoch 1400 | Loss: 0.0477\n",
      "Epoch 1600 | Loss: 0.0447\n",
      "Epoch 1800 | Loss: 0.0489\n",
      "Epoch 2000 | Loss: 0.0412\n",
      "Epoch 2200 | Loss: 0.0415\n",
      "Epoch 2400 | Loss: 0.0426\n",
      "Epoch 2600 | Loss: 0.0541\n",
      "Epoch 2800 | Loss: 0.0427\n",
      "Epoch 3000 | Loss: 0.0474\n",
      "Epoch 3200 | Loss: 0.0402\n",
      "Epoch 3400 | Loss: 0.0484\n",
      "Epoch 3600 | Loss: 0.0495\n",
      "Epoch 3800 | Loss: 0.0570\n",
      "Epoch 4000 | Loss: 0.0489\n",
      "Epoch 4200 | Loss: 0.0578\n",
      "Epoch 4400 | Loss: 0.0652\n",
      "Epoch 4600 | Loss: 0.0481\n",
      "Epoch 4800 | Loss: 0.0433\n",
      "sigmal = 1.0\n",
      "Epoch 0 | Loss: 1.1542\n",
      "Epoch 200 | Loss: 0.3850\n",
      "Epoch 400 | Loss: 0.3755\n",
      "Epoch 600 | Loss: 0.4853\n",
      "Epoch 800 | Loss: 0.4757\n",
      "Epoch 1000 | Loss: 0.4610\n",
      "Epoch 1200 | Loss: 0.5469\n",
      "Epoch 1400 | Loss: 0.6124\n",
      "Epoch 1600 | Loss: 0.4664\n",
      "Epoch 1800 | Loss: 0.4615\n",
      "Epoch 2000 | Loss: 0.4042\n",
      "Epoch 2200 | Loss: 0.4608\n",
      "Epoch 2400 | Loss: 0.4150\n",
      "Epoch 2600 | Loss: 0.5001\n",
      "Epoch 2800 | Loss: 0.5087\n",
      "Epoch 3000 | Loss: 0.4873\n",
      "Epoch 3200 | Loss: 0.4324\n",
      "Epoch 3400 | Loss: 0.4446\n",
      "Epoch 3600 | Loss: 0.4153\n",
      "Epoch 3800 | Loss: 0.4111\n",
      "Epoch 4000 | Loss: 0.4187\n",
      "Epoch 4200 | Loss: 0.4707\n",
      "Epoch 4400 | Loss: 0.4120\n",
      "Epoch 4600 | Loss: 0.4687\n",
      "Epoch 4800 | Loss: 0.5524\n",
      "sigmal = 10.0\n",
      "Epoch 0 | Loss: 110.7179\n",
      "Epoch 200 | Loss: 1.1103\n",
      "Epoch 400 | Loss: 1.0849\n",
      "Epoch 600 | Loss: 1.0375\n",
      "Epoch 800 | Loss: 1.0250\n",
      "Epoch 1000 | Loss: 1.1523\n",
      "Epoch 1200 | Loss: 1.1405\n",
      "Epoch 1400 | Loss: 1.1764\n",
      "Epoch 1600 | Loss: 1.0902\n",
      "Epoch 1800 | Loss: 1.0512\n",
      "Epoch 2000 | Loss: 1.0714\n",
      "Epoch 2200 | Loss: 1.1990\n",
      "Epoch 2400 | Loss: 0.9954\n",
      "Epoch 2600 | Loss: 0.9576\n",
      "Epoch 2800 | Loss: 1.0309\n",
      "Epoch 3000 | Loss: 1.0273\n",
      "Epoch 3200 | Loss: 1.1400\n",
      "Epoch 3400 | Loss: 1.1278\n",
      "Epoch 3600 | Loss: 1.1196\n",
      "Epoch 3800 | Loss: 1.1368\n",
      "Epoch 4000 | Loss: 1.0577\n",
      "Epoch 4200 | Loss: 0.9400\n",
      "Epoch 4400 | Loss: 1.1348\n",
      "Epoch 4600 | Loss: 1.1380\n",
      "Epoch 4800 | Loss: 1.0340\n"
     ]
    }
   ],
   "source": [
    "# --------------------- Training loop across different noise levels ---------------------\n",
    "exps = []   # To store explained variance ratios\n",
    "losses = [] # To record loss values during training\n",
    "\n",
    "for sigmal in sigmals:\n",
    "    # Define a new neural network (as a list of layers) for each noise level\n",
    "    linears = [nn.Linear(nn_widths[i], nn_widths[i + 1]) for i in range(hidden_depth + 1)]\n",
    "    parameters = [p for layer in linears for p in layer.parameters()]  # Collect all trainable parameters\n",
    "\n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Choose optimizer\n",
    "    optimizer = optim.Adam(parameters, lr=lr) if opt == \"Adam\" else optim.SGD(parameters, lr=lr)\n",
    "\n",
    "    print(f\"sigmal = {sigmal}\")\n",
    "\n",
    "    # --------------------- Training the denoising network ---------------------\n",
    "    for j in range(epoch):\n",
    "        # Randomly sample a batch of training data\n",
    "        choices = np.random.choice(n_train, batch_size)\n",
    "        perturb = torch.normal(0, sigmal, size=(batch_size, input_dim))  # Add Gaussian noise\n",
    "        inputs0 = torch.tensor(xs[choices], dtype=torch.float) + perturb  # Noisy input\n",
    "\n",
    "        # Forward pass through each layer using LeakyReLU\n",
    "        x = inputs0\n",
    "        act = nn.LeakyReLU(slope)\n",
    "        for i in range(hidden_depth):\n",
    "            x = act(linears[i](x))\n",
    "        outputs = linears[hidden_depth](x)  # Final layer (no activation)\n",
    "\n",
    "        # Training objective: recover the *negative* of the noise added\n",
    "        loss = criterion(outputs, -perturb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(float(loss.data))\n",
    "\n",
    "        if j % log == 0:\n",
    "            print(f\"Epoch {j} | Loss: {loss:.4f}\")\n",
    "\n",
    "    # --------------------- Perform stochastic walks using trained network ---------------------\n",
    "    x0 = copy.deepcopy(xs[int(n_train / 2)])  # Start from a central data point\n",
    "    x0 = x0[np.newaxis, :]\n",
    "    x0 = x0 + np.random.randn(n_walk, input_dim) * sigmal  # Add noise\n",
    "\n",
    "    # Pass walk samples through trained denoising network\n",
    "    x = torch.tensor(x0, dtype=torch.float)\n",
    "    for i in range(hidden_depth):\n",
    "        x = act(linears[i](x))\n",
    "    x = linears[hidden_depth](x)\n",
    "    x0 = x0 + x.detach().numpy()  # Apply learned correction\n",
    "\n",
    "    # --------------------- Compute explained variance ratio via PCA ---------------------\n",
    "    pca = PCA()\n",
    "    pca.fit(x0)\n",
    "    svs = pca.singular_values_\n",
    "    exp_ratio = svs**2 / np.sum(svs**2)\n",
    "    exps.append(exp_ratio)\n",
    "\n",
    "# Stack results into a NumPy array\n",
    "exps = np.array(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1753368803861,
     "user": {
      "displayName": "Peter Wulff",
      "userId": "00945079592507225542"
     },
     "user_tz": -120
    },
    "id": "VFR_JDSu5qDK",
    "outputId": "652ec47b-f12c-40f6-feaf-8c325da29338"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-6-4039673660.py:25: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n",
      "  plt.clf()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------- Define helper function to compute effective rank (ERD) ---------------------\n",
    "def f(x, a=2):\n",
    "    n = x.shape[1]\n",
    "    mask = x < 1 / (a * n)\n",
    "    return np.sum(np.cos(np.pi / 2 * n * a * x) * mask, axis=1)\n",
    "\n",
    "# --------------------- Plot explained variance and effective dimensionality ---------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "# Plot each PCA component’s explained variance\n",
    "for i in range(input_dim):\n",
    "    plt.plot(sigmals, exps[:, i], marker=\"o\")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel(r\"$L$\", fontsize=25)\n",
    "plt.ylabel(\"Explained Ratio\", fontsize=25)\n",
    "\n",
    "# Plot effective dimensionality (ERD) on second y-axis\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(sigmals, f(exps, a=2), marker=\"o\", color=\"red\", linewidth=5, ls=\"--\", markersize=15)\n",
    "plt.ylabel(r\"$m_{eff}$\", fontsize=25, color=\"red\")\n",
    "\n",
    "# Save the plot to disk\n",
    "plt.savefig(os.path.join(dir_path, 'ERD.png').replace(\"\\\\\", \"/\"), bbox_inches=\"tight\")\n",
    "plt.clf()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPyYGs+aAQM35ct1MsMMrwN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:Orange]",
   "language": "python",
   "name": "conda-env-Orange-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
